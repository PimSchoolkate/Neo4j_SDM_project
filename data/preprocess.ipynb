{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "from lorem_text import lorem\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "direc = os.getcwd()\n",
    "unprocessed_data_path = os.path.join(direc, 'unprocessed')\n",
    "processed_data_path = os.path.join(direc,'processed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_keywords(df):\n",
    "        key_words = []\n",
    "        db_key_words = ['data management', 'indexing', 'data modeling', 'big data', 'data processing', 'data storage', 'data querying']\n",
    "        it = 0\n",
    "        for key, value in df['title:string'].iteritems():\n",
    "                it += 1\n",
    "                try:\n",
    "                        possible_keywords = value.split()\n",
    "\n",
    "                        sep = '|'\n",
    "                        if it == 5:\n",
    "                                key_words.append(sep.join(random.sample(db_key_words, 3)))\n",
    "                                it = 0\n",
    "                        elif len(possible_keywords) <= 3:\n",
    "                                key_words.append(sep.join(possible_keywords))\n",
    "                        else:\n",
    "                                key_words.append(sep.join(random.sample(possible_keywords, 3)))\n",
    "                except:\n",
    "                        key_words.append(value)\n",
    "        df[\"keywords\"] = key_words\n",
    "        return df\n",
    "\n",
    "def create_abstract(df):\n",
    "        abstracts = []\n",
    "        for row in range(df.shape[0]):\n",
    "                length = random.randint(3,10)\n",
    "                abstracts.append(lorem.words(length))\n",
    "        df[\"abstract:string\"] = abstracts\n",
    "        return df\n",
    "\n",
    "\n",
    "def split_list_of_authors(df_column):\n",
    "    authors = []\n",
    "    for x in list(df_column):\n",
    "        try:\n",
    "            authors.extend(x.split('|')) \n",
    "        except:\n",
    "            pass\n",
    "    return authors\n",
    "\n",
    "def create_citations(article_df, proceeding_df):\n",
    "        article_ids = list(article_df['article:ID'])\n",
    "        proceed_paper_ids = list(proceeding_df['inproceedings:ID'])\n",
    "        citations_for_art = []\n",
    "        citations_for_bk = []\n",
    "        sep = '|'\n",
    "        for key, value in article_df['article:ID'].iteritems():\n",
    "                citation_list = []\n",
    "                amount = random.randint(0,15)\n",
    "                for citation in range(amount):\n",
    "                        book_or_article = random.randint(0,1)\n",
    "                        if book_or_article == 0:\n",
    "                                cit = random.choice(article_ids)\n",
    "                                if cit != value:\n",
    "                                        citation_list.append(str(cit))\n",
    "                                else:\n",
    "                                        pass\n",
    "                        else:\n",
    "                                cit = random.choice(proceed_paper_ids)\n",
    "                                if cit != value:\n",
    "                                        citation_list.append(str(cit))\n",
    "                                else:\n",
    "                                        pass\n",
    "                citations_for_art.append(sep.join(citation_list))\n",
    "        for key, value in proceeding_df['inproceedings:ID'].iteritems():\n",
    "                citation_list = []\n",
    "                amount = random.randint(0,15)\n",
    "                for citation in range(amount):\n",
    "                        book_or_article = random.randint(0,1)\n",
    "                        if book_or_article == 0:\n",
    "                                cit = random.choice(article_ids)\n",
    "                                if cit != value:\n",
    "                                        citation_list.append(str(cit))\n",
    "                                else:\n",
    "                                        pass\n",
    "                        else:\n",
    "                                cit = random.choice(proceed_paper_ids)\n",
    "                                if cit != value:\n",
    "                                        citation_list.append(str(cit))\n",
    "                                else:\n",
    "                                        pass\n",
    "                citations_for_bk.append(sep.join(citation_list))\n",
    "        \n",
    "        article_df['citations:string[]'] = citations_for_art\n",
    "        proceeding_df['citations:string[]'] = citations_for_bk\n",
    "        return(article_df, proceeding_df)\n",
    "\n",
    "def choose_corresponding_author(df):\n",
    "        corresponding_list = []\n",
    "        co_author_list = []\n",
    "        for key, value in df['author:string[]'].iteritems():\n",
    "                try:\n",
    "                        authors = value.split('|')\n",
    "                        corr = random.choice(authors)\n",
    "                        corresponding_list.append(corr)\n",
    "                        authors.remove(corr)\n",
    "                        co_authors = authors\n",
    "                        co_authors = '|'.join(co_authors)\n",
    "                        co_author_list.append(co_authors)\n",
    "\n",
    "                except:\n",
    "                        corresponding_list.append(value)\n",
    "                        co_author_list.append(\"\")\n",
    "\n",
    "        df['corresponding'] = corresponding_list\n",
    "        df['co_authors'] = co_author_list\n",
    "        return df\n",
    "\n",
    "def create_reviewers(article_df, proceeding_df):\n",
    "        author = split_list_of_authors(article_df['author:string[]']) + split_list_of_authors(proceeding_df['author:string[]'])\n",
    "        article_reviewers = []\n",
    "        proceeding_paper_reviewer = []\n",
    "        sep = '|'\n",
    "        for key, value in article_df['author:string[]'].iteritems():\n",
    "                try:\n",
    "                        authors = value.split('|')\n",
    "                except:\n",
    "                        authors = ['']\n",
    "                review_list = []\n",
    "                amount = random.randint(1,4)\n",
    "                for rev in range(amount):\n",
    "                        reviewer = random.choice(author)\n",
    "                        if reviewer not in authors:\n",
    "                                review_list.append(reviewer)\n",
    "                        else:\n",
    "                                pass\n",
    "                article_reviewers.append(sep.join(review_list))\n",
    "        for key, value in proceeding_df['author:string[]'].iteritems():\n",
    "                try:\n",
    "                        authors = value.split('|')\n",
    "                except:\n",
    "                        authors = ['']\n",
    "                review_list = []\n",
    "                amount = random.randint(1,4)\n",
    "                #proceeding_df['location'].iloc[key] = locations[proceeding_df['inproceedings:ID'].iloc[key]]\n",
    "                for rev in range(amount):\n",
    "                        reviewer = random.choice(author)\n",
    "                        if reviewer not in authors:\n",
    "                                review_list.append(reviewer)\n",
    "                        else:\n",
    "                                pass\n",
    "                proceeding_paper_reviewer.append(sep.join(review_list))\n",
    "        \n",
    "        article_df['reviewed_by:string[]'] = article_reviewers\n",
    "        proceeding_df['reviewed_by:string[]'] = proceeding_paper_reviewer\n",
    "        return(article_df, proceeding_df)\n",
    "\n",
    "def join_proceeding_data(proceeding_df, inproceeding_df):\n",
    "        res = inproceeding_df.merge(proceeding_df, how = 'inner', on='booktitle:string')\n",
    "        columns = res.columns\n",
    "        new_columns = []\n",
    "        for col in columns:\n",
    "                if '_x' in col:\n",
    "                        new_columns.append('inproc' + col)\n",
    "                elif '_y' in col:\n",
    "                        new_columns.append('proc' + col)\n",
    "                else:\n",
    "                        new_columns.append(col)\n",
    "        res.columns = new_columns\n",
    "        return res\n",
    "\n",
    "def create_location_dict(proceeding_df):\n",
    "        cities = ['Antwerp', 'Brussels', 'Twente', 'Utrecht', 'Oosterbeek', 'Schaarbeek', 'The Hague', 'Barcelona', 'Kviv', 'Marioepol', 'Odessa'\n",
    "        ,'Charkov', 'Stockholm', 'Te Anua', 'Aarschot', 'Llanfairpwllgwyngyllgogerychwyrndrobwllllantysiliogogogoch']\n",
    "        proc_cit = {}\n",
    "        location_list = []\n",
    "        for inproceedings in np.unique(proceeding_df['procmdate']):\n",
    "                proc_cit[inproceedings] = random.choice(cities)\n",
    "        for key, value in proceeding_df['procmdate'].iteritems():\n",
    "                location_list.append(proc_cit[value])\n",
    "        proceeding_df['location'] = location_list\n",
    "        return proceeding_df\n",
    "\n",
    "def join_author_data(author_df, df):\n",
    "        author_df = author_df.rename(columns={'author:string':'corresponding', ':ID' : 'correspondingID'})\n",
    "        res = df.merge(author_df, how='inner', on='corresponding')\n",
    "        return res\n",
    "\n",
    "\n",
    "def remove_format_spec(df):\n",
    "        new_cols = []\n",
    "        for col in df.columns:\n",
    "                new_cols.append(col.split(':')[0])\n",
    "        df.columns = new_cols\n",
    "        return df\n",
    "\n",
    "def read_and_sample(input_path, random_state = 1):\n",
    "    list_of_files = os.listdir(input_path)\n",
    "    for file in list_of_files:\n",
    "        file_path = os.path.join(input_path, file)\n",
    "        if 'header' in file:\n",
    "                column_names = (pd.read_csv(file_path, sep = ';', header=0).columns)\n",
    "                data_file = file_path.replace('_header', '')\n",
    "                data = pd.read_csv(data_file, sep = ';', header=None, names = column_names)\n",
    "                if 'article' in file:\n",
    "                        data = data.sample(frac = 0.01, random_state = random_state)\n",
    "                        article_data = data\n",
    "                        article_data = article_data[[c for c in article_data if article_data[c].isna().sum() < 0.95*article_data.shape[0]]]\n",
    "                elif 'inproceedings' in file:\n",
    "                        data = data.sample(frac = 0.01, random_state = random_state)\n",
    "                        inproceedings_data = data\n",
    "                        inproceedings_data = inproceedings_data[[c for c in inproceedings_data if inproceedings_data[c].isna().sum() < 0.95*inproceedings_data.shape[0]]]\n",
    "                elif 'proceedings' in file:\n",
    "                        data = data.sample(frac = 0.1, random_state = random_state)\n",
    "                        proceedings_data = data\n",
    "                        proceedings_data = proceedings_data[['proceedings:ID', 'booktitle:string','ee:string[]', 'isbn:string[]'\n",
    "                                                ,'mdate:date', 'publisher:string[]', 'series:string[]', 'volume:string']]\n",
    "        elif file == 'dblp_author.csv':\n",
    "                author_data = pd.read_csv(file_path, sep = ';', header=0)\n",
    "\n",
    "    return(author_data, article_data, proceedings_data, inproceedings_data)\n",
    "\n",
    "def preprocess_pipeline(input_path, output_path, random_state = 1):\n",
    "    author_data, article_data, proceedings_data, inproceedings_data = read_and_sample(input_path = input_path, random_state=random_state)\n",
    "    article_data = create_keywords(article_data)\n",
    "    inproceedings_data = create_keywords(inproceedings_data)\n",
    "    article_data = create_abstract(article_data)\n",
    "    inproceedings_data = create_abstract(inproceedings_data)\n",
    "    article_data, inproceedings_data = create_citations(article_data, inproceedings_data)\n",
    "    article_data, inproceedings_data = create_reviewers(article_data, inproceedings_data)\n",
    "    article_data = choose_corresponding_author(article_data)\n",
    "    inproceedings_data = choose_corresponding_author(inproceedings_data)\n",
    "    article_data = join_author_data(author_data, article_data)\n",
    "    paper_proceedings_data = join_proceeding_data(proceedings_data, inproceedings_data)\n",
    "    article_data = remove_format_spec(article_data)\n",
    "    paper_proceedings_data = remove_format_spec(paper_proceedings_data)\n",
    "    paper_proceedings_data = create_location_dict(paper_proceedings_data)\n",
    "    article_data.to_csv(os.path.join(output_path, 'article.csv'), sep = ';', index=None)\n",
    "    paper_proceedings_data.to_csv(os.path.join(output_path, 'proceeding_papers.csv'), sep = ';', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\louis\\AppData\\Local\\Temp\\ipykernel_6920\\1051388280.py:194: DtypeWarning: Columns (2,4,5,6,7,8,9,10,11,13,14,18,19,20,21,22,23,24,25,26,30,31,33) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(data_file, sep = ';', header=None, names = column_names)\n",
      "C:\\Users\\louis\\AppData\\Local\\Temp\\ipykernel_6920\\1051388280.py:194: DtypeWarning: Columns (2,5,13,18,29) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(data_file, sep = ';', header=None, names = column_names)\n",
      "C:\\Users\\louis\\AppData\\Local\\Temp\\ipykernel_6920\\1051388280.py:194: DtypeWarning: Columns (4,6,7,14,15,17,18) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(data_file, sep = ';', header=None, names = column_names)\n",
      "C:\\Users\\louis\\AppData\\Local\\Temp\\ipykernel_6920\\1051388280.py:194: DtypeWarning: Columns (2,5,6,7,9,10,16,17,18,19,21,22,25,26) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(data_file, sep = ';', header=None, names = column_names)\n",
      "C:\\Users\\louis\\AppData\\Local\\Temp\\ipykernel_6920\\1051388280.py:194: DtypeWarning: Columns (5,10,13,16,20,21,23,24) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(data_file, sep = ';', header=None, names = column_names)\n",
      "C:\\Users\\louis\\AppData\\Local\\Temp\\ipykernel_6920\\1051388280.py:194: DtypeWarning: Columns (1,2,4,5,10,12,13,18,19,22,23,26,27) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(data_file, sep = ';', header=None, names = column_names)\n",
      "C:\\Users\\louis\\AppData\\Local\\Temp\\ipykernel_6920\\1051388280.py:194: DtypeWarning: Columns (2,3,4,5,6,15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(data_file, sep = ';', header=None, names = column_names)\n"
     ]
    }
   ],
   "source": [
    "preprocess_pipeline(input_path=unprocessed_data_path, output_path=processed_data_path, random_state=1)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "edc68bf9a5f422fe4f7b8b393f086b6041829635654dfaef4252eac2caf5d030"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('NEO4J')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
